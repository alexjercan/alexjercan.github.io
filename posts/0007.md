---
title: Solving Metazooa with Information Theory
description: Finding the optimal starting guess in the Metazooa game using entropy and large-scale simulations.
template: post
date: Tue, 10 Feb 2026 11:00:31 -0000
tags: [information-theory, algorithms, game-theory, mathematics]
---

Have you ever played [Metazooa](https://metazooa.com/)?

On the surface it looks simple. The game chooses an animal, and you try to guess it. But instead of asking yes/no questions, you are constrained to taxonomy. Every guess is a species, and every answer is the *lowest common ancestor* (LCA) between your guess and the secret animal.

If the game is thinking of a lion and you guess a whale, the response is "Mammalia".

Helpful.
But not *that* helpful.

After a few rounds, a natural question starts to come up:

**Is there an objectively best animal to start with?**

Not a decent guess.
Not an intuitive guess.
The optimal one.

It turns out this question leads straight into information theory.

---

## The Game Is a Tree Problem

At any moment in Metazooa, the game state is a tree of remaining possibilities. Each leaf is a species. Each internal node is a taxonomic clade.

When you make a guess, the LCA partitions the remaining animals into groups. Some guesses barely narrow things down. Others split the tree almost in half.

So the real question is not:

> What animal feels reasonable?

It is:

> Which guess reshapes the search space the most?

That is exactly what information theory measures.

---

## Measuring Uncertainty with Entropy

If there are \( N \) equally likely animals remaining, the uncertainty is:

$$
H = \log_2(N)
$$

In the dataset I analyzed, there are 328 species:

$$
H_{\text{initial}} \approx 8.36 \text{ bits}
$$

This number has a concrete interpretation. It is the minimum average number of yes/no questions required to identify the correct animal.

No strategy can beat this lower bound.

Each guess reduces uncertainty. The amount it reduces it by is called **information gain**:

$$
\text{Information Gain} = H_{\text{before}} - H_{\text{after}}
$$

So the optimal starting guess is simply the one that maximizes expected information gain.

---

## Brute Forcing the Best Guess

For a fixed guess, we can simulate every possible hidden animal.

1. Compute the LCA between the guess and every target.
2. Group animals by the resulting LCA.
3. Convert group sizes into probabilities.
4. Compute the entropy of this partition.

The expected entropy after the guess is:

$$
H_{\text{after}} = - \sum_i p_i \log_2(p_i)
$$

where each \( p_i \) corresponds to one possible LCA response.

This gives a single score for that guess: its expected information gain.

Repeat this for all 328 species, and rank them.

### Entropy Rankings

The top entropy maximizing guesses are:

- **Mink**
- **Stoat**
- **Otter**
- **Ferret**
- **Weasel**

All of them sit in structurally balanced regions of the taxonomy.

Interestingly, the differences between the top candidates are extremely small. Once you are in a good part of the tree, performance differences shrink dramatically.

---

## Minmax: A Different Philosophy

Entropy optimizes the average case.

But what if we care about the worst case?

That leads to a different scoring rule:

For each guess, compute the LCA buckets and look only at the largest one. Choose the guess that minimizes that worst case branch size.

Same partitioning.
Different objective.

Instead of arguing theoretically about which is better, I ran large scale simulations.

---

## 1000-Game Simulations

For every possible starting guess, I simulated 1000 full games.

Each time:

- Pick a random hidden animal.
- Play using the minmax strategy.
- Count how many guesses it takes to win.

Then average the results.

### Minmax Results (Empirical)

The best performers were:

- **Bison** — 4.78 average guesses
- **Water buffalo** — 4.79
- **Dog** — 4.79
- **Yak** — 4.79
- **Skunk** — 4.80

And many more within a few hundredths.

Two things stand out:

1. The rankings are not identical to entropy.
2. The differences are tiny.

Mink, which tops the entropy list, is still near the top under minmax — just not number one.

---

## What This Tells Us

Entropy and minmax are different ideas.

One cares about the average.
The other cares about the worst case.

But in practice, they end up favoring animals in roughly the same part of the taxonomy.

Why?

Because there are only so many ways to split the tree well.

Once you sit in a structurally balanced region, both strategies reward you.

The exact scoring formula matters less than the geometry of the tree itself.

---

## A Practical Tool

After building all this, I created a small web tool to automate the strategy:

**[Metazooa Analysis Tool](https://alexjercan.github.io/metazooa/)**

You can:

- Choose entropy or minmax
- Restrict to a clade like "Mammalia"
- Remove animals you already ruled out
- Recompute the best next guess instantly

It essentially plays the mathematically optimal move given the current state.

Educational cheating.

---

## Final Takeaways

- Metazooa is a decision tree problem in disguise.
- Entropy gives a clean, principled way to score guesses.
- Minmax offers a different but closely related perspective.
- In structured search problems, geometry dominates intuition.

So what is the best first guess?

If you want the entropy winner: try **mink**.

If you prefer empirical minmax performance: **bison** or **water buffalo** are excellent.

In reality, any of the top-ranked animals will perform almost identically.

And once you start looking at problems this way, you begin to see entropy everywhere.

---

*Full source code, simulation scripts, and rankings are available here:*
**[github.com/alexjercan/metazooa](https://github.com/alexjercan/metazooa)**

